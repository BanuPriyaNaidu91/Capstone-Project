\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Capstone\_Week2\_Pjt},
            pdfauthor={Banu Priya},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Capstone\_Week2\_Pjt}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Banu Priya}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{24th Sep 2019}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\begin{document}
\maketitle

\#\# Synopsis

This is the Milestone Report for week 2 of the Coursera Data Science
Capstone project.

The objective of the report is to understand the various statistical
propertties of dataset to build a prediction model for the final data
product using Shiny Application.

The model will be trained using unified document corpus from the
following sources:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Blogs
\item
  News
\item
  Twitter
\end{enumerate}

The provided text data has four different languages. This project will
only focus on the English language.

\hypertarget{environment-setup}{%
\subsection{Environment Setup}\label{environment-setup}}

Loading initial packages and clearing the global workspace (including
hidden objects).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(knitr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'knitr' was built under R version 3.6.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rm}\NormalTok{(}\DataTypeTok{list =} \KeywordTok{ls}\NormalTok{(}\DataTypeTok{all.names =} \OtherTok{TRUE}\NormalTok{))}
\KeywordTok{setwd}\NormalTok{(}\StringTok{"C:/Users/b.s.priya/Documents/Data Science_R/Capstone"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{loading-the-data}{%
\subsection{Loading the Data}\label{loading-the-data}}

Download, unzip and load the training data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainURL <-}\StringTok{ "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"}
\NormalTok{trainDataFile <-}\StringTok{ "data/Coursera-SwiftKey.zip"}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(}\StringTok{'data'}\NormalTok{)) \{}
  \KeywordTok{dir.create}\NormalTok{(}\StringTok{'data'}\NormalTok{)}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(}\StringTok{"C:/Users/b.s.priya/Documents/Data Science_R/Capstone"}\NormalTok{)) \{}
\NormalTok{  tempFile <-}\StringTok{ }\KeywordTok{tempfile}\NormalTok{()}
  \KeywordTok{download.file}\NormalTok{(trainURL, tempFile)}
  \KeywordTok{unzip}\NormalTok{(tempFile, }\DataTypeTok{exdir =} \StringTok{"data"}\NormalTok{)}
  \KeywordTok{unlink}\NormalTok{(tempFile)}
\NormalTok{\}}

\CommentTok{## blogs}
\NormalTok{blogsFileName <-}\StringTok{ "C:/Users/b.s.priya/Documents/Data Science_R/Capstone/en_US.blogs.txt"}
\NormalTok{con <-}\StringTok{ }\KeywordTok{file}\NormalTok{(blogsFileName, }\DataTypeTok{open =} \StringTok{"r"}\NormalTok{)}
\NormalTok{blogs <-}\StringTok{ }\KeywordTok{readLines}\NormalTok{(con, }\DataTypeTok{encoding =} \StringTok{"UTF-8"}\NormalTok{, }\DataTypeTok{skipNul =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{close}\NormalTok{(con)}

\CommentTok{## news}
\NormalTok{newsFileName <-}\StringTok{ "C:/Users/b.s.priya/Documents/Data Science_R/Capstone/en_US.news.txt"}
\NormalTok{con <-}\StringTok{ }\KeywordTok{file}\NormalTok{(newsFileName, }\DataTypeTok{open =} \StringTok{"r"}\NormalTok{)}
\NormalTok{news <-}\StringTok{ }\KeywordTok{readLines}\NormalTok{(con, }\DataTypeTok{encoding =} \StringTok{"UTF-8"}\NormalTok{, }\DataTypeTok{skipNul =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in readLines(con, encoding = "UTF-8", skipNul = TRUE): incomplete
## final line found on 'C:/Users/b.s.priya/Documents/Data Science_R/Capstone/
## en_US.news.txt'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{close}\NormalTok{(con)}

\CommentTok{## twitter}
\NormalTok{twitterFileName <-}\StringTok{ "C:/Users/b.s.priya/Documents/Data Science_R/Capstone/en_US.twitter.txt"}
\NormalTok{con <-}\StringTok{ }\KeywordTok{file}\NormalTok{(twitterFileName, }\DataTypeTok{open =} \StringTok{"r"}\NormalTok{)}
\NormalTok{twitter <-}\StringTok{ }\KeywordTok{readLines}\NormalTok{(con, }\DataTypeTok{encoding =} \StringTok{"UTF-8"}\NormalTok{, }\DataTypeTok{skipNul =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{close}\NormalTok{(con)}
\KeywordTok{rm}\NormalTok{(con)}
\end{Highlighting}
\end{Shaded}

\hypertarget{basic-summary-of-data}{%
\subsection{Basic Summary of Data}\label{basic-summary-of-data}}

Prior to building the unified document corpus and cleaning the data, a
basic summary of the three text corpora is being provided which includes
file sizes, number of lines, number of characters, and number of words
for each source file. Also included are basic statistics on the number
of words per line (min, mean, and max).

\hypertarget{initial-data-summary}{%
\subsubsection{Initial Data Summary}\label{initial-data-summary}}

\begin{verbatim}
## Warning: package 'kableExtra' was built under R version 3.6.1
\end{verbatim}

\begin{table}[t]

\caption{\label{tab:initial-data-summary-table}}
\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
File & FileSize & Lines & Characters & Words & WPL.Min & WPL.Mean & WPL.Max\\
\hline
en\_US.blogs.txt & 200  MB & 899288 & 206824505 & 37570839 & 0 & 42 & 6726\\
\hline
en\_US.news.txt & 196  MB & 77259 & 15639408 & 2651432 & 1 & 35 & 1123\\
\hline
en\_US.twitter.txt & 159  MB & 2360148 & 162096241 & 30451170 & 1 & 13 & 47\\
\hline
\end{tabular}
\end{table}

An initial investigation of the data shows that on average, each text
has a relatively low number of words per line on. Blogs tend to have
more words per line, followed by news and then twitter which has the
least words per line. The lower number of words per line for the Twitter
data is expected given that a tweet is limited to a certain number of
characters. Even when Twitter doubled its character count from 140 to
280 characters in 2017, research shows that only 1\% of tweets hit the
280-character limit, and only 12\% of tweets are longer than 140
characters. Perhaps after so many years, users were simply trained to
the 140-character limit.

Another important observation in this initial investigation shows that
the text files are fairly large. To improve processing time, a sample
size of 1\% will be obtained from all three data sets and then combined
into a unified document corpus for subsequent analyses later in this
report as part of preparing the data.

\hypertarget{histogram-of-words-per-line}{%
\subsubsection{Histogram of Words per
Line}\label{histogram-of-words-per-line}}

\begin{verbatim}
## Warning: package 'gridExtra' was built under R version 3.6.1
\end{verbatim}

\includegraphics{figures/initial-data-summary-plot-1.pdf}

The relatively low number of words in the three source files charted
earlier in this section is also visible in the histogram plots shown
above. This observation seems to support a general trend towards short
and concise communications that may be useful later in the project.

\hypertarget{prepare-the-data}{%
\subsection{Prepare the Data}\label{prepare-the-data}}

Prior to performing exploratory data analysis, the three data sets will
be sampled at 1\% to improve performance. All non-English characters
will be removed from the subset of data and then combined into a single
data set. The combined sample data set will be written to disk which
contains 33,365 lines and 702,126 words.

The next step is to create a corpus from the sampled data set. A custom
function named \texttt{buildCorpus} will be employed to perform the
following transformation steps for each document:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remove URL, Twitter handles and email patterns by converting them to
  spaces using a custom content transformer
\item
  Convert all words to lowercase
\item
  Remove common English stop words
\item
  Remove punctuation marks
\item
  Remove numbers
\item
  Trim whitespace
\item
  Remove profanity
\item
  Convert to plain text documents
\end{enumerate}

The corpus will then be written to disk in two formats: a serialized R
object in RDS format and as a text file. Finally, the first 10 documents
(lines) from the corpus will be displayed.

\begin{table}[t]

\caption{\label{tab:prepare-the-data-build-corpus}First 10 Documents}
\begin{tabular}{l}
\hline
freezer\\
\hline
liquid bottom bin usually run food decomposing can often smell sour can acidic plants recommend use liquid water plants problem still liquid needs dealt sometimes just dilute lot water still water plants yes may bit acidic better dumping drain even worse letting run balcony onto balconies yuck\\
\hline
backs similar popular set except yellow instead green vertical just typical stats biography fun trivia questions answerssee many can get right\\
\hline
chapter filing betsey johnson llc will see chains stores close person familiar matter said steven madden ltd owns betsey johnson license likely will keep four five flagships new york city cities person said\\
\hline
fold unfinished edge front back panel place velcro folded edges sew sew velcro top panel onto wrong side fabric velcro bottom panel onto right side fabric sunsuit facing velcro pieces facing\\
\hline
difference can viewed quantitatively also come undone garnered little yellow post slightly less recent selection book ruth acquired backback leaf\\
\hline
dont think authentic leadership skill position essential anyone wants bring best work life\\
\hline
fast\\
\hline
gave authority speak mankind know fact many humans dedicated pissdrinkers convinced fluids therapeutic properties celebrated exponent practice english actress sarah miles met many years ago circus show\\
\hline
chairman joint chiefs staff gen martin dempsey ordered inquiry april day canceled perspectives islam islamic radicalism training course asserted islam war west course offered elective joint forces staff college norfolk va since\\
\hline
\end{tabular}
\end{table}

\hypertarget{appendix}{%
\subsubsection{Appendix}\label{appendix}}

\hypertarget{a.1-basic-data-summary}{%
\subsubsection{A.1 Basic Data Summary}\label{a.1-basic-data-summary}}

Basic summary of the three text corpora.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(stringi)}
\KeywordTok{library}\NormalTok{(kableExtra)}
\CommentTok{# assign sample size}
\NormalTok{sampleSize =}\StringTok{ }\FloatTok{0.01}
\CommentTok{# file size}
\NormalTok{fileSizeMB <-}\StringTok{ }\KeywordTok{round}\NormalTok{(}\KeywordTok{file.info}\NormalTok{(}\KeywordTok{c}\NormalTok{(blogsFileName,}
\NormalTok{                                newsFileName,}
\NormalTok{                                twitterFileName))}\OperatorTok{$}\NormalTok{size }\OperatorTok{/}\StringTok{ }\DecValTok{1024} \OperatorTok{^}\StringTok{ }\DecValTok{2}\NormalTok{)}
\CommentTok{# num lines per file}
\NormalTok{numLines <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\KeywordTok{list}\NormalTok{(blogs, news, twitter), length)}
\CommentTok{# num characters per file}
\NormalTok{numChars <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\KeywordTok{nchar}\NormalTok{(blogs), }\KeywordTok{nchar}\NormalTok{(news), }\KeywordTok{nchar}\NormalTok{(twitter)), sum)}
\CommentTok{# num words per file}
\NormalTok{numWords <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\KeywordTok{list}\NormalTok{(blogs, news, twitter), stri_stats_latex)[}\DecValTok{4}\NormalTok{,]}
\CommentTok{# words per line}
\NormalTok{wpl <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{list}\NormalTok{(blogs, news, twitter), }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{stri_count_words}\NormalTok{(x))}
\CommentTok{# words per line summary}
\NormalTok{wplSummary =}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\KeywordTok{list}\NormalTok{(blogs, news, twitter),}
                    \ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{summary}\NormalTok{(}\KeywordTok{stri_count_words}\NormalTok{(x))[}\KeywordTok{c}\NormalTok{(}\StringTok{'Min.'}\NormalTok{, }\StringTok{'Mean'}\NormalTok{, }\StringTok{'Max.'}\NormalTok{)])}
\KeywordTok{rownames}\NormalTok{(wplSummary) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'WPL.Min'}\NormalTok{, }\StringTok{'WPL.Mean'}\NormalTok{, }\StringTok{'WPL.Max'}\NormalTok{)}
\NormalTok{summary <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}
  \DataTypeTok{File =} \KeywordTok{c}\NormalTok{(}\StringTok{"en_US.blogs.txt"}\NormalTok{, }\StringTok{"en_US.news.txt"}\NormalTok{, }\StringTok{"en_US.twitter.txt"}\NormalTok{),}
  \DataTypeTok{FileSize =} \KeywordTok{paste}\NormalTok{(fileSizeMB, }\StringTok{" MB"}\NormalTok{),}
  \DataTypeTok{Lines =}\NormalTok{ numLines,}
  \DataTypeTok{Characters =}\NormalTok{ numChars,}
  \DataTypeTok{Words =}\NormalTok{ numWords,}
  \KeywordTok{t}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(}\KeywordTok{round}\NormalTok{(wplSummary)))}
\NormalTok{)}
\KeywordTok{kable}\NormalTok{(summary,}
      \DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{,}
      \DataTypeTok{align =} \KeywordTok{c}\NormalTok{(}\StringTok{"l"}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\StringTok{"r"}\NormalTok{, }\DecValTok{7}\NormalTok{)),}
      \DataTypeTok{caption =} \StringTok{""}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{position =} \StringTok{"left"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{a.2-histogram-of-words-per-line}{%
\subsubsection{A.2 Histogram of Words per
Line}\label{a.2-histogram-of-words-per-line}}

Histogram of words per line for the three text corpora.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(gridExtra)}
\NormalTok{plot1 <-}\StringTok{ }\KeywordTok{qplot}\NormalTok{(wpl[[}\DecValTok{1}\NormalTok{]],}
               \DataTypeTok{geom =} \StringTok{"histogram"}\NormalTok{,}
               \DataTypeTok{main =} \StringTok{"US Blogs"}\NormalTok{,}
               \DataTypeTok{xlab =} \StringTok{"Words per Line"}\NormalTok{,}
               \DataTypeTok{ylab =} \StringTok{"Frequency"}\NormalTok{,}
               \DataTypeTok{binwidth =} \DecValTok{5}\NormalTok{)}
\NormalTok{plot2 <-}\StringTok{ }\KeywordTok{qplot}\NormalTok{(wpl[[}\DecValTok{2}\NormalTok{]],}
               \DataTypeTok{geom =} \StringTok{"histogram"}\NormalTok{,}
               \DataTypeTok{main =} \StringTok{"US News"}\NormalTok{,}
               \DataTypeTok{xlab =} \StringTok{"Words per Line"}\NormalTok{,}
               \DataTypeTok{ylab =} \StringTok{"Frequency"}\NormalTok{,}
               \DataTypeTok{binwidth =} \DecValTok{5}\NormalTok{)}
\NormalTok{plot3 <-}\StringTok{ }\KeywordTok{qplot}\NormalTok{(wpl[[}\DecValTok{3}\NormalTok{]],}
               \DataTypeTok{geom =} \StringTok{"histogram"}\NormalTok{,}
               \DataTypeTok{main =} \StringTok{"US Twitter"}\NormalTok{,}
               \DataTypeTok{xlab =} \StringTok{"Words per Line"}\NormalTok{,}
               \DataTypeTok{ylab =} \StringTok{"Frequency"}\NormalTok{,}
               \DataTypeTok{binwidth =} \DecValTok{1}\NormalTok{)}
\NormalTok{plotList =}\StringTok{ }\KeywordTok{list}\NormalTok{(plot1, plot2, plot3)}
\KeywordTok{do.call}\NormalTok{(grid.arrange, }\KeywordTok{c}\NormalTok{(plotList, }\KeywordTok{list}\NormalTok{(}\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{a.3-sample-and-clean-the-data}{%
\subsubsection{A.3 Sample and Clean the
Data}\label{a.3-sample-and-clean-the-data}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set seed for reproducability}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{101}\NormalTok{)}

\CommentTok{# sample all three data sets}
\NormalTok{sampleBlogs <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(blogs, }\KeywordTok{length}\NormalTok{(blogs) }\OperatorTok{*}\StringTok{ }\NormalTok{sampleSize, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{sampleNews <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(news, }\KeywordTok{length}\NormalTok{(news) }\OperatorTok{*}\StringTok{ }\NormalTok{sampleSize, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{sampleTwitter <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(twitter, }\KeywordTok{length}\NormalTok{(twitter) }\OperatorTok{*}\StringTok{ }\NormalTok{sampleSize, }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)}

\CommentTok{# remove all non-English characters from the sampled data}
\NormalTok{sampleBlogs <-}\StringTok{ }\KeywordTok{iconv}\NormalTok{(sampleBlogs, }\StringTok{"latin1"}\NormalTok{, }\StringTok{"ASCII"}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{)}
\NormalTok{sampleNews <-}\StringTok{ }\KeywordTok{iconv}\NormalTok{(sampleNews, }\StringTok{"latin1"}\NormalTok{, }\StringTok{"ASCII"}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{)}
\NormalTok{sampleTwitter <-}\StringTok{ }\KeywordTok{iconv}\NormalTok{(sampleTwitter, }\StringTok{"latin1"}\NormalTok{, }\StringTok{"ASCII"}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{)}

\CommentTok{# combine all three data sets into a single data set and write to disk}
\NormalTok{sampleData <-}\StringTok{ }\KeywordTok{c}\NormalTok{(sampleBlogs, sampleNews, sampleTwitter)}
\NormalTok{sampleDataFileName <-}\StringTok{ "C:/Users/b.s.priya/Documents/Data Science_R/Capstone/en_US.sample.txt"}
\NormalTok{con <-}\StringTok{ }\KeywordTok{file}\NormalTok{(sampleDataFileName, }\DataTypeTok{open =} \StringTok{"w"}\NormalTok{)}
\KeywordTok{writeLines}\NormalTok{(sampleData, con)}
\KeywordTok{close}\NormalTok{(con)}

\CommentTok{# get number of lines and words from the sample data set}
\NormalTok{sampleDataLines <-}\StringTok{ }\KeywordTok{length}\NormalTok{(sampleData);}
\NormalTok{sampleDataWords <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{stri_count_words}\NormalTok{(sampleData))}

\CommentTok{# remove variables no longer needed to free up memory}
\KeywordTok{rm}\NormalTok{(blogs, news, twitter, sampleBlogs, sampleNews, sampleTwitter)}
\end{Highlighting}
\end{Shaded}

\hypertarget{a.4-build-corpus}{%
\subsubsection{A.4 Build Corpus}\label{a.4-build-corpus}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tm)}
\CommentTok{# download bad words file}
\NormalTok{badWordsURL <-}\StringTok{ "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words_text-file_2018_07_30.zip"}
\NormalTok{badWordsFile <-}\StringTok{ "C:/Users/b.s.priya/Documents/Data Science_R/Capstone/full-list-of-bad-words_text-file_2018_07_30.txt"}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(}\StringTok{'data'}\NormalTok{)) \{}
  \KeywordTok{dir.create}\NormalTok{(}\StringTok{'data'}\NormalTok{)}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{file.exists}\NormalTok{(badWordsFile)) \{}
\NormalTok{  tempFile <-}\StringTok{ }\KeywordTok{tempfile}\NormalTok{()}
  \KeywordTok{download.file}\NormalTok{(badWordsURL, tempFile)}
  \KeywordTok{unzip}\NormalTok{(tempFile, }\DataTypeTok{exdir =} \StringTok{"data"}\NormalTok{)}
  \KeywordTok{unlink}\NormalTok{(tempFile)}
\NormalTok{\}}
\NormalTok{buildCorpus <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{ (dataSet) \{}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{VCorpus}\NormalTok{(}\KeywordTok{VectorSource}\NormalTok{(dataSet))}
\NormalTok{  toSpace <-}\StringTok{ }\KeywordTok{content_transformer}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x, pattern) }\KeywordTok{gsub}\NormalTok{(pattern, }\StringTok{" "}\NormalTok{, x))}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, toSpace, }\StringTok{"(f|ht)tp(s?)://(.*)[.][a-z]+"}\NormalTok{)}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, toSpace, }\StringTok{"@[^}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s]+"}\NormalTok{)}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, toSpace, }\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{b[A-Z a-z 0-9._ - ]*[@](.*?)[.]\{1,3\} }\CharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{)}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, tolower)}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, removeWords, }\KeywordTok{stopwords}\NormalTok{(}\StringTok{"english"}\NormalTok{))}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, removePunctuation)}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, removeNumbers)}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, stripWhitespace)}
  \CommentTok{# remove profane words from the sample data set}
\NormalTok{  con <-}\StringTok{ }\KeywordTok{file}\NormalTok{(badWordsFile, }\DataTypeTok{open =} \StringTok{"r"}\NormalTok{)}
\NormalTok{  profanity <-}\StringTok{ }\KeywordTok{readLines}\NormalTok{(con, }\DataTypeTok{encoding =} \StringTok{"UTF-8"}\NormalTok{, }\DataTypeTok{skipNul =} \OtherTok{TRUE}\NormalTok{)}
  \KeywordTok{close}\NormalTok{(con)}
\NormalTok{  profanity <-}\StringTok{ }\KeywordTok{iconv}\NormalTok{(profanity, }\StringTok{"latin1"}\NormalTok{, }\StringTok{"ASCII"}\NormalTok{, }\DataTypeTok{sub =} \StringTok{""}\NormalTok{)}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, removeWords, profanity)}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, removeWords, }\KeywordTok{c}\NormalTok{(}\StringTok{"cyalisx"}\NormalTok{,}
                                      \StringTok{"cyberfuc"}\NormalTok{,}
                                      \StringTok{"cyberfuck"}\NormalTok{,}
                                      \StringTok{"cyberfucked"}\NormalTok{,}
                                      \StringTok{"cyberfucker"}\NormalTok{,}
                                      \StringTok{"cyberfuckers"}\NormalTok{,}
                                      \StringTok{"cyberfucking"}\NormalTok{))}
  
  \CommentTok{# convert the text to plaintext}
\NormalTok{  docs <-}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(docs, PlainTextDocument)}
  \KeywordTok{return}\NormalTok{(docs)}
\NormalTok{\}}


\CommentTok{# build the corpus and write to disk (RDS)}
\NormalTok{corpus <-}\StringTok{ }\KeywordTok{buildCorpus}\NormalTok{(sampleData)}
\KeywordTok{saveRDS}\NormalTok{(corpus, }\DataTypeTok{file =} \StringTok{"C:/Users/b.s.priya/Documents/Data Science_R/Capstone/en_US.corpus.rds"}\NormalTok{)}
\CommentTok{# convert corpus to a dataframe and write lines/words to disk (text)}
\NormalTok{corpusText <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{text =} \KeywordTok{unlist}\NormalTok{(}\KeywordTok{sapply}\NormalTok{(corpus, }\StringTok{'['}\NormalTok{, }\StringTok{"content"}\NormalTok{)), }\DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{con <-}\StringTok{ }\KeywordTok{file}\NormalTok{(}\StringTok{"C:/Users/b.s.priya/Documents/Data Science_R/Capstone/en_US.corpus.txt"}\NormalTok{, }\DataTypeTok{open =} \StringTok{"w"}\NormalTok{)}
\KeywordTok{writeLines}\NormalTok{(corpusText}\OperatorTok{$}\NormalTok{text, con)}
\KeywordTok{close}\NormalTok{(con)}
\KeywordTok{kable}\NormalTok{(}\KeywordTok{head}\NormalTok{(corpusText}\OperatorTok{$}\NormalTok{text, }\DecValTok{10}\NormalTok{),}
      \DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{,}
      \DataTypeTok{col.names =} \OtherTok{NULL}\NormalTok{,}
      \DataTypeTok{align =} \KeywordTok{c}\NormalTok{(}\StringTok{"l"}\NormalTok{),}
      \DataTypeTok{caption =} \StringTok{"First 10 Documents"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{position =} \StringTok{"left"}\NormalTok{)}
\CommentTok{# remove variables no longer needed to free up memory}
\KeywordTok{rm}\NormalTok{(sampleData)}
\end{Highlighting}
\end{Shaded}


\end{document}
